{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhav6412/datascience_ml/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nV6GPjnv4Tr"
      },
      "source": [
        "#code source: http://occam.olin.edu/sites/default/files/DataScienceMaterials/machine_learning_lecture_2/Machine%20Learning%20Lecture%202.html\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loCNKQkInG5Z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import *\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model datasets\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgVQrPInGFi-",
        "outputId": "d50b0ad9-5027-4200-93d1-22f4eaab4290"
      },
      "source": [
        "\n",
        "#Using GridSearchCV\n",
        "data = load_breast_cancer() #refer: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
        "\n",
        "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2]}]\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=.9)\n",
        "model = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'f1', cv=5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.best_estimator_)\n",
        "print(model.score(X_test, y_test))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "0.9411764705882354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8YonYLCqg7m",
        "outputId": "c99c138a-c4cc-4501-c485-77cc5847ccb2"
      },
      "source": [
        "data = load_breast_cancer()\n",
        "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2]}]\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size = .9)\n",
        "model = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'f1', cv = 5)\n",
        "model.fit(X_train, y_train)\n",
        "print(model.best_estimator_)\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "0.9166666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPddwVwNT_lM"
      },
      "source": [
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS1jJuodv4Ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6071e421-49db-40eb-f76c-2d83ca70127a"
      },
      "source": [
        "# More Sparsity (Fewer elements of W* being non-zero) by increasing Lambda (decreasing C) \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "clf = LogisticRegression(C=0.1, penalty='l1',  solver = 'liblinear');\n",
        "clf.fit(X_train, y_train);\n",
        "w = clf.coef_\n",
        "print(np.count_nonzero(w))\n",
        "y = clf.predict_proba\n",
        "print(y)\n",
        "\n",
        "# 1. model.coef_ gives the weights of every feature in our fitted data\n",
        "# (1D array)\n",
        "# 2. model.predict_prob(X) gives the probability of every datapoint of X belonging to a particular class\n",
        "# (2D array)\n",
        "# # 3. model.decision_function(X) gives How confident is the model on about which class every datapoint belongs to. Closer to reg line means confidence is low, while farther away from reg line, the confidence is high\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "<bound method LogisticRegression.predict_proba of LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b86z5PSvfAk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWF0yjcQv4T0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e170e829-e190-4528-f31d-b640e4a1487f"
      },
      "source": [
        "clf = LogisticRegression(C=0.01, penalty='l1', solver = 'liblinear');\n",
        "clf.fit(X_train, y_train);\n",
        "w = clf.coef_\n",
        "print(np.count_nonzero(w))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTDDmXxdv4T2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caad44b9-4e79-4f24-92a0-6eebcffd3d4b"
      },
      "source": [
        "clf = LogisticRegression(C=0.001, penalty='l1', solver = 'liblinear');\n",
        "clf.fit(X_train, y_train);\n",
        "w = clf.coef_\n",
        "print(np.count_nonzero(w))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH0VDyVuv4T5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e280099-0b7f-4310-d162-fe59b8382d50"
      },
      "source": [
        "clf = LogisticRegression(C=10, penalty='l1', solver = 'liblinear');\n",
        "clf.fit(X_train, y_train);\n",
        "w = clf.coef_\n",
        "print(np.count_nonzero(w))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZMzThodv4T7"
      },
      "source": [
        "'W' is computed internally and also it could be returned. The internal way of computing weight vector happens on the basis of optimization which is discussed in the next chapter. The optimization process has been discussed in later chapters. \n",
        "The weight components indicate the feature importance and these components are given by coef_ attribute.\n",
        " 1. model.coef_ gives the weights of every feature in our fitted data\n",
        " (1D array)\n",
        " 2. model.predict_prob(X) gives the probability of every datapoint of X belonging to a particular class\n",
        " (2D array)\n",
        " 3. model.decision_function(X) gives How confident is the model on about which class every datapoint belongs to. Closer to reg line means confidence is low, while farther away from reg line, the confidence is high\n",
        "\n",
        "\n",
        "LBFGS: It approximates the second derivative matrix updates with gradient evaluations. It stores only the last few updates, so it saves memory. It isn't super fast with large data sets. It will be the default solver as of Scikit-learn version 0.22.0.\n",
        "\n",
        "Liblinear: Library for Large Linear Classification. Uses a coordinate descent algorithm. Coordinate descent is based on minimizing a multivariate function by solving univariate optimization problems in a loop. In other words, it moves toward the minimum in one direction at a time. It is the default solver for Scikit-learn versions earlier than 0.22.0. It performs pretty well with high dimensionality. It does have a number of drawbacks. It can get stuck, is unable to run in parallel, and can only solve multi-class logistic regression with one-vs-rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqV_6ceM7Og-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}